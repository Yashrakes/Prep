## What is latency?

- _Latency_ is the amount of time a system takes to respond to a user request. Let’s say you send a request to an app to fetch an image and the system takes 2 seconds to respond to your request. The latency of the system is 2 seconds.

- Minimum latency is what efficient software systems strive for. No matter how much the traffic load on a system builds up, the latency should not go up. This is what scalability is.

- If the latency remains the same, we can say that the application scaled well with the increased load and is highly scalable.

- Let’s think of scalability in terms of _Big-O notation_. Ideally, the complexity of a system or an algorithm should be _O(1)_ which is _constant time_ like in a Key-value database.

- A program with the complexity of _O(n^2)_ where n is the size of the data set is not scalable. As the size of the data set increases the system will need more computational power to process the tasks.

---

## Measuring latency

Latency is measured as the time difference between the action that the user takes on the website. It can be an event like the click of a button, and the system response in reaction to that event.

This latency is generally divided into two parts:

1.  Network latency
2.  Application latency

![[Pasted image 20211007202953.png]]

<br>

### Network latency

Network latency is the amount of time that the network takes to send a data packet from point A to point B. The network should be efficient enough to handle the increased traffic load on the website. To cut down the network latency, businesses use CDN and try to deploy their servers across the globe as close to the end-user as possible.

### Application latency

Application latency is the amount of time the application takes to process a user request. There are more than a few ways to cut down the application latency. The first step is to run stress and load tests on the application and scan for the bottlenecks that slow down the system as a whole. We’ll talk more about it in the upcoming lesson.

---

## Why is low latency so important for online services?
- Latency plays a major role in determining if an online business wins or loses a customer. Nobody likes to wait for a response on a website. There is a well-known saying, “if you want to test a person’s patience, give them a slow internet connection.”

- If the visitor gets the response within a stipulated time, great otherwise they’ll bounce off to another website.

- There is ample market research that concludes high latency in applications is a big factor in customers bouncing off a website. If there is money involved, zero latency is what businesses want. If only if this was possible.

- Think of massive multiplayer online (MMO) games. A slight lag in an in-game event ruins the whole experience. A gamer with a high latency internet connection will have a slow response time despite having the best reaction time of all the players in an arena.

- Algorithmic trading services need to process events within milliseconds. Fintech companies have dedicated networks to run low latency trading. The regular network just won’t cut it.

- We can realize the importance of low latency by the fact that in 2011 [Huawei and Hibernia Atlantic started laying a fiber-optic link cable across the Atlantic Ocean between London and New York](https://www.telegraph.co.uk/technology/news/8753784/The-300m-cable-that-will-save-traders-milliseconds.html). This property was estimated to cost approximately $300M just to save traders 6 milliseconds of latency.

---