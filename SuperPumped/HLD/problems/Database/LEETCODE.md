
## The Core Problem LeetCode Solves

On the surface a coding platform seems simple â€” store problems, run code, show rankings. But consider the real constraints:

```
LeetCode scale reality:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active users:              20 million+
Daily submissions:         500,000+ (6 submissions/second sustained)
Peak submissions:          2,000 submissions/second (during contests)
Total problems:            3,000+
Test cases per problem:    50-200 (some have 1,000+)
Code execution time:       0.1s - 10s per test case
Leaderboard updates:       Real-time (sub-second)

Requirements:
â†’ Store problem descriptions, constraints, examples
â†’ Execute untrusted user code safely (sandbox isolation)
â†’ Run code against 100+ test cases per submission
â†’ Store billions of historical submissions for analytics
â†’ Update user rankings instantly after submission
â†’ Show global leaderboard (<100ms query)
â†’ Support multiple languages (Python, Java, C++, Go, Rust, etc.)
â†’ Handle contest spikes (10,000 users submitting simultaneously)
â†’ Prevent cheating (plagiarism detection needs historical code)
â†’ Query: "Show me all my Python submissions for problem #42"
```

This combination of **relational problem data + massive time-series submission writes + code execution isolation + real-time ranking updates + historical code search** is what forces this multi-database architecture.

---

## Why PostgreSQL for Problems?

### The Problem Data Structure

```
WHAT IS A "PROBLEM"?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Problem #42: "Two Sum"
{
  problem_id: 42,
  title: "Two Sum",
  difficulty: "Easy",
  description: "Given an array of integers...",
  constraints: "2 <= nums.length <= 10^4",
  examples: [
    {input: "[2,7,11,15], target=9", output: "[0,1]"},
    {input: "[3,2,4], target=6", output: "[1,2]"}
  ],
  test_cases: [
    {input: "[2,7,11,15], 9", expected: "[0,1]"},
    {input: "[3,2,4], 6", expected: "[1,2]"},
    ... (100+ test cases)
  ],
  hints: ["Use a hash map", "One-pass solution exists"],
  tags: ["Array", "Hash Table"],
  companies: ["Amazon", "Google", "Microsoft"],
  acceptance_rate: 0.48,
  related_problems: [1, 15, 167],
  solution_code: {...},
  editorial: "Approach 1: Brute Force...",
  created_at: "2018-01-15",
  updated_at: "2024-02-20"
}
```

This is highly relational data with complex structure.

### Why PostgreSQL Is Perfect

```
POSTGRESQL SCHEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Problems table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ title     â”‚ difficulty â”‚ description        â”‚ constraints â”‚ acceptance_rate
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ Two Sum   â”‚ Easy       â”‚ Given an array...  â”‚ 2 <= n...   â”‚ 0.48
15         â”‚ 3Sum      â”‚ Medium     â”‚ Given an integer...â”‚ 3 <= n...   â”‚ 0.31
1          â”‚ Add Two...â”‚ Medium     â”‚ You are given...   â”‚ 1 <= l...   â”‚ 0.39

Additional columns:
  slug: VARCHAR (URL-friendly: "two-sum")
  created_at: TIMESTAMP
  updated_at: TIMESTAMP
  is_premium: BOOLEAN (locked for free users)


Test_Cases table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_case_id â”‚ problem_id â”‚ input                â”‚ expected_output â”‚ is_hidden
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1001         â”‚ 42         â”‚ "[2,7,11,15]\n9"     â”‚ "[0,1]"         â”‚ false
1002         â”‚ 42         â”‚ "[3,2,4]\n6"         â”‚ "[1,2]"         â”‚ false
1003         â”‚ 42         â”‚ "[100000,...]\n..." â”‚ "[999,1000]"    â”‚ true

is_hidden = true â†’ not shown to user, only used for judging


Problem_Tags (many-to-many):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ tag_name
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ Array
42         â”‚ Hash Table
15         â”‚ Array
15         â”‚ Two Pointers


Problem_Companies (many-to-many):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ company_name â”‚ frequency
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ Amazon       â”‚ 150
42         â”‚ Google       â”‚ 89
15         â”‚ Microsoft    â”‚ 120


Related_Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ related_id â”‚ relationship_type
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ 167        â”‚ similar
42         â”‚ 1          â”‚ follow_up
```

### Complex Queries PostgreSQL Handles Naturally

```
QUERY 1: Find problems by multiple criteria
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"Show me all Medium difficulty problems
 tagged 'Array' and 'Hash Table'
 asked by Amazon
 with acceptance rate > 30%"

SELECT p.problem_id, p.title, p.acceptance_rate
FROM problems p
JOIN problem_tags pt1 ON p.problem_id = pt1.problem_id
JOIN problem_tags pt2 ON p.problem_id = pt2.problem_id
JOIN problem_companies pc ON p.problem_id = pc.problem_id
WHERE p.difficulty = 'Medium'
AND pt1.tag_name = 'Array'
AND pt2.tag_name = 'Hash Table'
AND pc.company_name = 'Amazon'
AND p.acceptance_rate > 0.30
ORDER BY pc.frequency DESC;

â†’ Multiple JOINs
â†’ Complex filtering
â†’ Natural in SQL
â†’ <50ms with proper indexes


QUERY 2: Recommendation engine
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

"User just solved problem #42, what should they try next?"

SELECT rp.related_id, p.title, p.difficulty
FROM related_problems rp
JOIN problems p ON rp.related_id = p.problem_id
WHERE rp.problem_id = 42
AND rp.relationship_type = 'follow_up'
AND p.difficulty IN (
  SELECT difficulty FROM problems WHERE problem_id = 42
  UNION
  SELECT CASE 
    WHEN difficulty = 'Easy' THEN 'Medium'
    WHEN difficulty = 'Medium' THEN 'Hard'
    ELSE 'Hard'
  END
)
ORDER BY p.acceptance_rate DESC
LIMIT 5;

â†’ Subqueries, CASE statements
â†’ Complex logic
â†’ Impossible in NoSQL without application code
```

### Why Not MongoDB for Problems?

```
MONGODB ATTEMPT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Store problem as document:
{
  _id: 42,
  title: "Two Sum",
  difficulty: "Easy",
  test_cases: [
    {input: "...", expected: "..."},
    ... (100+ test cases embedded)
  ],
  tags: ["Array", "Hash Table"],
  companies: [{name: "Amazon", frequency: 150}, ...]
}

Pros:
âœ“ Flexible schema
âœ“ Single document fetch

Cons:
âœ— Cannot query efficiently: "problems with tags X AND Y"
   â†’ Must fetch all, filter in app
âœ— Cannot JOIN with submissions, user stats
âœ— Document size explosion (100+ test cases per problem)
   â†’ 16MB document limit risk
âœ— No enforced relationships (foreign keys)
âœ— Updating acceptance rate requires full document rewrite


When MongoDB works:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Simple key-value lookups
â†’ No complex queries needed
â†’ Schema changes frequently

LeetCode needs relational queries
PostgreSQL is the right choice
```

---

## Why Cassandra for Submissions?

### The Submission Write Pattern

```
SUBMISSION CHARACTERISTICS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Write pattern:
- 6 submissions/second sustained
- 2,000 submissions/second during contests
- Pure append (never update old submissions)
- Time-series data (timestamp critical)

Read pattern:
- "Show user's submission history" (by user_id, time)
- "Show all submissions for problem #42" (by problem_id, time)
- Rarely query old submissions (>1 month)
- Recent submissions queried frequently

Data volume:
- 500,000 submissions/day Ã— 365 = 182M submissions/year
- Each submission: ~10KB (code, result, metadata)
- Total: 1.8TB/year

This is a WRITE-HEAVY time-series workload
Perfect for Cassandra
```

### Cassandra Schema Design

```
CASSANDRA SCHEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Submissions_By_User table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_id â”‚ submission_time        â”‚ submission_id â”‚ problem_id â”‚ language â”‚ result â”‚ runtime â”‚ code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_001â”‚ 2024-02-26 10:00:00   â”‚ sub_abc       â”‚ 42         â”‚ python   â”‚ AC     â”‚ 52ms    â”‚ "def..."
user_001â”‚ 2024-02-26 09:55:00   â”‚ sub_xyz       â”‚ 42         â”‚ python   â”‚ WA     â”‚ N/A     â”‚ "def..."
user_001â”‚ 2024-02-25 15:30:00   â”‚ sub_def       â”‚ 15         â”‚ java     â”‚ AC     â”‚ 38ms    â”‚ "class..."

PRIMARY KEY ((user_id), submission_time, submission_id)
CLUSTERING ORDER BY (submission_time DESC, submission_id DESC)

Why this schema:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Partition key: user_id
â†’ All user's submissions in same partition
â†’ Efficient query: "show user's history"

Clustering key: submission_time (DESC)
â†’ Submissions sorted by time within partition
â†’ Most recent first
â†’ Efficient query: "show last 20 submissions"


Submissions_By_Problem table:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ submission_time      â”‚ submission_id â”‚ user_id  â”‚ language â”‚ result â”‚ runtime
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ 2024-02-26 10:00:00 â”‚ sub_abc       â”‚ user_001 â”‚ python   â”‚ AC     â”‚ 52ms
42         â”‚ 2024-02-26 09:58:00 â”‚ sub_ghi       â”‚ user_002 â”‚ java     â”‚ TLE    â”‚ N/A
42         â”‚ 2024-02-26 09:55:00 â”‚ sub_xyz       â”‚ user_001 â”‚ python   â”‚ WA     â”‚ N/A

PRIMARY KEY ((problem_id), submission_time, submission_id)

Why duplicate data (denormalization):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cassandra has no JOINs
Must query by partition key
Different queries need different partition keys
â†’ Duplicate data in multiple tables
â†’ Each table optimized for its query pattern

Storage is cheap
Write throughput is cheap (Cassandra strength)
Duplication is acceptable tradeoff
```

### Why Cassandra Handles This Better Than PostgreSQL

```
POSTGRESQL PROBLEMS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Submissions table (single table):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
submission_id â”‚ user_id  â”‚ problem_id â”‚ submitted_at        â”‚ result â”‚ code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sub_abc       â”‚ user_001 â”‚ 42         â”‚ 2024-02-26 10:00:00 â”‚ AC     â”‚ "def..."
sub_xyz       â”‚ user_001 â”‚ 42         â”‚ 2024-02-26 09:55:00 â”‚ WA     â”‚ "def..."
...
(182 million rows after 1 year)


Query: "Show user's last 20 submissions"
SELECT * FROM submissions
WHERE user_id = 'user_001'
ORDER BY submitted_at DESC
LIMIT 20;

Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ B-tree index on (user_id, submitted_at) required
â†’ Index size grows linearly with data
â†’ After 182M rows: index is 50GB+
â†’ Query requires index scan + sort
â†’ Latency: 50-200ms (acceptable but not ideal)

At 2,000 submissions/second during contest:
â†’ 2,000 INSERTs/second
â†’ Each INSERT updates B-tree index
â†’ Index maintenance becomes bottleneck
â†’ Vacuum lag (MVCC overhead)
â†’ Write latency spikes to 500ms+
â†’ Users see "Submission delayed" errors


CASSANDRA SOLUTION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Writes:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write to Cassandra:
1. Append to commit log (sequential disk write)
2. Write to MemTable (in-memory)
3. Acknowledge immediately

Write latency: <5ms (even at 2K writes/sec)
No index maintenance
No locks
No vacuum


Reads:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT * FROM submissions_by_user
WHERE user_id = 'user_001'
ORDER BY submission_time DESC
LIMIT 20;

Cassandra internals:
1. Hash user_id to find partition (Node 3)
2. Go directly to Node 3
3. Read from SSTable (data already sorted by submission_time)
4. Return top 20

Read latency: <10ms
No index needed (data naturally sorted by clustering key)
Scales horizontally (add more nodes)
```

---

## Why Redis Sorted Sets for Leaderboard?

### The Real-Time Ranking Problem

```
REQUIREMENTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Global leaderboard:
- Rank users by total problems solved
- Update instantly on submission acceptance
- Show top 100 users (<50ms)
- Show user's rank: "You are #12,345 out of 20M users"

Contest leaderboard:
- Rank users by problems solved + time penalty
- Update in real-time (thousands watching)
- Must be correct (no stale data)
- Sub-second updates during contest


Naive SQL approach:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users table:
user_id â”‚ problems_solved â”‚ total_runtime
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_001â”‚ 450             â”‚ 123456
user_002â”‚ 892             â”‚ 98765
...
(20 million rows)

Query: "Top 100 users"
SELECT user_id, problems_solved, total_runtime
FROM users
ORDER BY problems_solved DESC, total_runtime ASC
LIMIT 100;

â†’ Full table scan or full index scan
â†’ Sort 20M rows
â†’ Query time: 5-10 seconds
â†’ Unacceptable


Query: "What is user_001's rank?"
SELECT COUNT(*) + 1 as rank
FROM users
WHERE problems_solved > (
  SELECT problems_solved FROM users WHERE user_id = 'user_001'
)
OR (problems_solved = (
  SELECT problems_solved FROM users WHERE user_id = 'user_001'
)
AND total_runtime < (
  SELECT total_runtime FROM users WHERE user_id = 'user_001'
));

â†’ Subqueries
â†’ Scan millions of rows
â†’ Query time: 3-5 seconds
â†’ Impossible at scale
```

### Redis Sorted Set Solution

```
REDIS SORTED SET FOR LEADERBOARD:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Global leaderboard:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key:   leaderboard:global
Type:  Sorted Set
Score: problems_solved Ã— 10^9 - total_runtime
Member: user_id

ZADD leaderboard:global 450000123456 "user_001"
     ^^^^ score = 450 Ã— 10^9 - 123456

Why this scoring formula:
- Primary: problems_solved (higher is better)
- Tiebreaker: total_runtime (lower is better)
- Combined into single score for sorting


Update on submission acceptance:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User solves new problem in 52ms

ZINCRBY leaderboard:global 999999948 "user_001"
                          ^^^^ 10^9 - 52

New score: 451000123404
(451 problems, total runtime 123456 + 52 = 123508)

Atomic operation
Automatically re-sorts
Time: <1ms


Query: "Top 100 users"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ZREVRANGE leaderboard:global 0 99 WITHSCORES

Returns:
[
  ("user_top1", 892000098765),
  ("user_top2", 891000234567),
  ...
  ("user_top100", 450000999999)
]

Query time: <5ms
O(log N + 100) = O(log 20M + 100) â‰ˆ 24 operations


Query: "What is user_001's rank?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ZREVRANK leaderboard:global "user_001"

Returns: 12344 (zero-indexed, so rank is 12,345)

Query time: <2ms
O(log N) = O(log 20M) â‰ˆ 24 skip list hops
```

### Contest Leaderboard (Separate Sorted Set)

```
CONTEST LEADERBOARD:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key:   contest:123:leaderboard
Score: (problems_solved Ã— 10^12) + time_penalty

time_penalty = sum of (submission_time - contest_start + 20min Ã— wrong_attempts)

User solves 3 problems:
Problem 1: Solved at 10 min, 0 wrong attempts
  â†’ penalty = 10

Problem 2: Solved at 25 min, 2 wrong attempts
  â†’ penalty = 25 + (20 Ã— 2) = 65

Problem 3: Solved at 45 min, 1 wrong attempt
  â†’ penalty = 45 + (20 Ã— 1) = 65

Total penalty: 10 + 65 + 65 = 140 minutes

Score: (3 Ã— 10^12) + 140 = 3000000000140

ZADD contest:123:leaderboard 3000000000140 "user_001"


Real-time updates:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
10,000 users watching contest leaderboard
Poll every 5 seconds: "Show me current standings"

ZREVRANGE contest:123:leaderboard 0 49 WITHSCORES

50 queries/second (10K users / 200)
Redis handles 100K+ queries/second easily
<5ms latency per query
No database bottleneck
```

---

## Code Execution Isolation

### The Untrusted Code Problem

````
SECURITY THREAT:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User submits malicious code:

Python submission:
```python
import os
os.system("rm -rf /")  # Delete server files
```

C++ submission:
```cpp
while(true) { fork(); }  // Fork bomb
```

Java submission:
```java
new File("/etc/passwd").delete();  // Delete system files
```

Without isolation:
â†’ User code runs on judge server
â†’ Can access file system
â†’ Can consume all CPU/memory
â†’ Can attack other users' code
â†’ Server compromised
````

### Containerized Execution Solution

```
DOCKER CONTAINER SANDBOX:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Execution flow:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User submits code
        â”‚
        â–¼
Submission Service:
1. Save code to Cassandra (async)
2. Publish to Kafka topic: "code_execution"
3. Return submission_id to user
4. User sees: "Submission queued..."
        â”‚
        â–¼
Execution Worker (consumer):
5. Consume from Kafka
6. Create isolated Docker container:
   docker run --rm \
     --cpus=1 \          # CPU limit
     --memory=256m \     # Memory limit
     --network=none \    # No network access
     --read-only \       # Read-only filesystem
     --user=nobody \     # Non-root user
     --timeout=10s \     # 10 second max
     judge-python:latest

7. Mount code and test cases (read-only)
8. Execute: python solution.py < input.txt
9. Capture stdout, stderr, exit code
10. Kill container after 10 seconds
11. Compare output with expected
12. Publish result to Kafka: "code_results"
        â”‚
        â–¼
Result Service:
9. Update submission in Cassandra (result: AC/WA/TLE)
10. Update leaderboard in Redis (if AC)
11. Push notification to user via WebSocket


Security guarantees:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Isolated filesystem (cannot access server files)
âœ“ No network (cannot attack other systems)
âœ“ Resource limits (cannot consume all CPU/memory)
âœ“ Timeout (cannot run forever)
âœ“ Ephemeral (container destroyed after execution)
âœ“ Read-only code mount (cannot modify test cases)
```

---

## Test Case Storage (Blob Storage)

### The Large Dataset Problem

```
PROBLEM WITH DATABASE STORAGE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Some problems have massive test cases:

Problem #1000: "Process Large Dataset"
Test case #150:
- Input: 100MB file (1 million integers)
- Expected output: 50MB file

Storing in PostgreSQL/Cassandra:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ 100MB BLOB in database row
â†’ Fetched into memory on every submission
â†’ Database bloat (1000 problems Ã— 100 cases Ã— 10MB avg = 1TB)
â†’ Slow queries (transferring 100MB over network)
â†’ Expensive (database storage costs 10x vs object storage)


Large test cases per problem:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Graph problems: 10,000 node graph â†’ 5MB adjacency list
String problems: 1 million character string â†’ 1MB
Array problems: 100,000 element array â†’ 500KB

Total test case storage needed: 10TB+
```

### S3/Blob Storage Solution

```
BLOB STORAGE ARCHITECTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PostgreSQL (metadata only):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_case_id â”‚ problem_id â”‚ s3_key                        â”‚ size_bytes â”‚ is_hidden
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1001         â”‚ 42         â”‚ "test-cases/42/input_1.txt"   â”‚ 150        â”‚ false
1002         â”‚ 42         â”‚ "test-cases/42/input_2.txt"   â”‚ 180        â”‚ false
1003         â”‚ 1000       â”‚ "test-cases/1000/large_1.txt" â”‚ 104857600  â”‚ true


S3 bucket structure:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
s3://leetcode-test-cases/
  test-cases/
    42/
      input_1.txt     (150 bytes)
      expected_1.txt  (20 bytes)
      input_2.txt     (180 bytes)
      expected_2.txt  (25 bytes)
    1000/
      large_1.txt     (100 MB)
      large_expected_1.txt (50 MB)


Execution worker flow:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Receive submission: problem_id = 1000

2. Query PostgreSQL for test case metadata:
   SELECT s3_key FROM test_cases WHERE problem_id = 1000

3. Download test cases from S3 (parallel):
   aws s3 cp s3://leetcode-test-cases/test-cases/1000/large_1.txt /tmp/
   
4. Mount test case in Docker container:
   docker run -v /tmp/large_1.txt:/input.txt:ro ...

5. Execute code with test case

6. Clean up (delete local test case copy)


Benefits:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Cheap storage ($0.023/GB/month vs $0.10+ for database)
âœ“ No database bloat
âœ“ Parallel downloads (faster than database fetch)
âœ“ CDN integration (CloudFront caches frequently used test cases)
âœ“ Versioning (S3 versioning for test case updates)
âœ“ Durability (S3 11-nines durability vs database backups)
```

---

## Complete Schema Architecture

```
POSTGRESQL SCHEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ slug      â”‚ title     â”‚ difficulty â”‚ acceptance_rate â”‚ is_premium â”‚ created_at
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ two-sum   â”‚ Two Sum   â”‚ Easy       â”‚ 0.48            â”‚ false      â”‚ 2018-01-15
15         â”‚ 3sum      â”‚ 3Sum      â”‚ Medium     â”‚ 0.31            â”‚ false      â”‚ 2018-02-01

Indexes:
  PRIMARY KEY (problem_id)
  UNIQUE (slug)
  INDEX (difficulty)
  INDEX (acceptance_rate)


Test_Cases:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
test_case_id â”‚ problem_id â”‚ s3_input_key   â”‚ s3_expected_key â”‚ is_hidden
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1001         â”‚ 42         â”‚ "tc/42/in1"    â”‚ "tc/42/exp1"    â”‚ false
1002         â”‚ 42         â”‚ "tc/42/in2"    â”‚ "tc/42/exp2"    â”‚ true

Indexes:
  PRIMARY KEY (test_case_id)
  INDEX (problem_id)


User_Problems (track solved problems):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_id  â”‚ problem_id â”‚ status      â”‚ language â”‚ best_runtime â”‚ last_attempted
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_001 â”‚ 42         â”‚ solved      â”‚ python   â”‚ 52ms         â”‚ 2024-02-26
user_001 â”‚ 15         â”‚ attempted   â”‚ java     â”‚ NULL         â”‚ 2024-02-25

Indexes:
  PRIMARY KEY (user_id, problem_id)
  INDEX (user_id, status)


CASSANDRA SCHEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Submissions_By_User:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_id â”‚ submission_time      â”‚ submission_id â”‚ problem_id â”‚ language â”‚ result â”‚ runtime â”‚ memory â”‚ code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_001â”‚ 2024-02-26 10:00:00 â”‚ sub_abc       â”‚ 42         â”‚ python   â”‚ AC     â”‚ 52ms    â”‚ 14MB   â”‚ "def..."
user_001â”‚ 2024-02-26 09:55:00 â”‚ sub_xyz       â”‚ 42         â”‚ python   â”‚ WA     â”‚ N/A     â”‚ N/A    â”‚ "def..."

PRIMARY KEY ((user_id), submission_time, submission_id)
CLUSTERING ORDER BY (submission_time DESC)
TTL: 90 days (old submissions auto-deleted)


Submissions_By_Problem:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
problem_id â”‚ submission_time    â”‚ submission_id â”‚ user_id  â”‚ result â”‚ runtime
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
42         â”‚ 2024-02-26 10:00:00â”‚ sub_abc       â”‚ user_001 â”‚ AC     â”‚ 52ms
42         â”‚ 2024-02-26 09:58:00â”‚ sub_ghi       â”‚ user_002 â”‚ TLE    â”‚ N/A

PRIMARY KEY ((problem_id), submission_time, submission_id)

Purpose: Analytics on problem difficulty (acceptance rates)


REDIS SCHEMA:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Global leaderboard:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key:   leaderboard:global
Type:  Sorted Set
Score: (problems_solved Ã— 10^9) - total_runtime_ms

ZADD leaderboard:global 450000123456 "user_001"


Contest leaderboard:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key:   contest:123:leaderboard
Score: (problems_solved Ã— 10^12) + time_penalty_minutes

ZADD contest:123:leaderboard 3000000000140 "user_001"


User stats cache:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key:   user:user_001:stats
Type:  Hash

HSET user:user_001:stats
  problems_solved "450"
  easy_solved "150"
  medium_solved "200"
  hard_solved "100"
  acceptance_rate "0.65"
  ranking "12345"

TTL: 1 hour (refresh periodically)


Submission result cache (temporary):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Key:   submission:sub_abc:result
Value: "PENDING" | "AC" | "WA" | "TLE" | "RE"
TTL:   60 seconds

WebSocket polling uses this
Avoids hammering Cassandra for result
```

---

## Complete Database Flow

```
FLOW 1: User Submits Code
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User writes solution and clicks "Submit"
        â”‚
        â–¼
POST /api/submit
{
  problem_id: 42,
  language: "python",
  code: "def twoSum(nums, target):\n  ..."
}
        â”‚
        â–¼
STEP 1: Validate and create submission
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Generate submission_id: sub_abc123
Current time: 2024-02-26 10:00:00

Write to Cassandra (async):
INSERT INTO submissions_by_user
(user_id, submission_time, submission_id, problem_id, language, code, result)
VALUES ('user_001', '2024-02-26 10:00:00', 'sub_abc123', 42, 'python', '...', 'PENDING');

Time: <5ms (write to commit log + MemTable)


STEP 2: Mark result as pending in Redis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SET submission:sub_abc123:result "PENDING" EX 60


STEP 3: Publish to Kafka
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Producer.send(
  topic="code_execution",
  key=submission_id,
  value={
    submission_id: "sub_abc123",
    user_id: "user_001",
    problem_id: 42,
    language: "python",
    code: "def twoSum...",
    timestamp: 1708945200
  }
)

Kafka buffers durably
Returns immediately


STEP 4: Return to user
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response:
{
  submission_id: "sub_abc123",
  status: "PENDING",
  message: "Your submission is being judged..."
}

User sees: "Running..."

Total API latency: <20ms
Code execution happens asynchronously
```

```
FLOW 2: Code Execution (Async)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Execution worker (Kafka consumer) picks up submission
        â”‚
        â–¼
STEP 1: Fetch test cases
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query PostgreSQL:
SELECT s3_input_key, s3_expected_key, is_hidden
FROM test_cases
WHERE problem_id = 42
ORDER BY test_case_id;

Returns: 150 test cases

Download from S3 (parallel, 10 at a time):
aws s3 cp s3://bucket/tc/42/in1 /tmp/tc1_in
aws s3 cp s3://bucket/tc/42/exp1 /tmp/tc1_exp
...

Time: ~500ms for all test cases


STEP 2: Create Docker container
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker create --name judge_sub_abc123 \
  --cpus=1 \
  --memory=256m \
  --network=none \
  --read-only \
  --user=nobody \
  judge-python:3.11


STEP 3: Mount code and test cases
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker cp /tmp/solution.py judge_sub_abc123:/solution.py
docker cp /tmp/tc1_in judge_sub_abc123:/input.txt


STEP 4: Execute code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
start_time = now()

docker start -a judge_sub_abc123 \
  timeout 10s python /solution.py < /input.txt > /output.txt

end_time = now()
runtime = end_time - start_time

Capture:
- stdout â†’ actual_output
- stderr â†’ error_messages
- exit_code


STEP 5: Compare output
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
expected = read("/tmp/tc1_exp")
actual = read_from_container("/output.txt")

if actual == expected:
  result = "AC" (Accepted)
elif timeout:
  result = "TLE" (Time Limit Exceeded)
elif runtime_error:
  result = "RE" (Runtime Error)
else:
  result = "WA" (Wrong Answer)


STEP 6: Repeat for all test cases
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For test cases 2-150:
  - Destroy old container
  - Create new container
  - Run code with next test case
  - Check result

Overall result:
- If ALL test cases AC â†’ Final result: AC
- If ANY test case fails â†’ Final result: First failure (WA/TLE/RE)

Total execution time: 5-30 seconds (depends on # test cases)


STEP 7: Publish result
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Producer.send(
  topic="code_results",
  key=submission_id,
  value={
    submission_id: "sub_abc123",
    user_id: "user_001",
    problem_id: 42,
    result: "AC",
    runtime: "52ms",
    memory: "14MB",
    test_cases_passed: 150,
    timestamp: now()
  }
)
```

```
FLOW 3: Result Processing
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Result consumer receives AC result
        â”‚
        â–¼
STEP 1: Update Cassandra
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UPDATE submissions_by_user
SET result = 'AC',
    runtime = '52ms',
    memory = '14MB'
WHERE user_id = 'user_001'
AND submission_time = '2024-02-26 10:00:00'
AND submission_id = 'sub_abc123';


STEP 2: Update PostgreSQL (user stats)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

Continue

21:40

```
STEP 2: Update PostgreSQL (user stats)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Check if first AC for this problem:
SELECT status FROM user_problems
WHERE user_id = 'user_001' AND problem_id = 42;

If not exists or status = 'attempted':
  -- First solve!
  INSERT INTO user_problems (user_id, problem_id, status, language, best_runtime, last_attempted)
  VALUES ('user_001', 42, 'solved', 'python', '52ms', NOW())
  ON CONFLICT (user_id, problem_id) 
  DO UPDATE SET 
    status = 'solved',
    best_runtime = LEAST(EXCLUDED.best_runtime, user_problems.best_runtime),
    last_attempted = NOW();
  
  -- Increment user's solved count
  UPDATE users
  SET problems_solved = problems_solved + 1,
      easy_solved = easy_solved + 1  -- if problem.difficulty = 'Easy'
  WHERE user_id = 'user_001';


STEP 3: Update Redis leaderboard
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
If first solve (new problem solved):
  
  ZINCRBY leaderboard:global 999999948 "user_001"
         ^^^^ score increment = 10^9 - runtime_ms
         
  New score reflects:
  - One more problem solved (10^9 added)
  - Total runtime increased by 52ms (52 subtracted)
  
  Leaderboard automatically re-sorts
  User's rank updates instantly


STEP 4: Update Redis cache
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SET submission:sub_abc123:result "AC" EX 60

HSET user:user_001:stats
  problems_solved "451"  â† incremented
  easy_solved "151"
  acceptance_rate "0.66"  â† recalculated

EXPIRE user:user_001:stats 3600


STEP 5: Notify user via WebSocket
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WebSocket broadcast to user_001:
{
  type: "SUBMISSION_RESULT",
  submission_id: "sub_abc123",
  result: "AC",
  runtime: "52ms",
  memory: "14MB",
  message: "Accepted! ğŸ‰",
  new_rank: 12344,
  rank_change: +1
}

User sees green checkmark instantly
Confetti animation plays
Rank updated in sidebar


STEP 6: Award achievements (if applicable)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Check milestones:
IF problems_solved = 100:
  INSERT INTO user_achievements (user_id, achievement_id, earned_at)
  VALUES ('user_001', 'century_club', NOW());
  
  Notify: "Achievement Unlocked: Century Club! ğŸ†"

IF first solve in new category:
  Award: "Array Master Badge"

Total result processing: <100ms
```

```
FLOW 4: User Views Leaderboard
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User opens "Leaderboard" page
        â”‚
        â–¼
GET /api/leaderboard?page=1
        â”‚
        â–¼
STEP 1: Fetch top 100 from Redis
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ZREVRANGE leaderboard:global 0 99 WITHSCORES

Returns:
[
  ("user_top1", 892000098765),
  ("user_top2", 891000234567),
  ("user_top3", 890000456789),
  ...
  ("user_top100", 450000999888)
]

Query time: <5ms


STEP 2: Decode scores
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each user:
  score = 892000098765
  problems_solved = score / 10^9 = 892
  total_runtime = 10^9 - (score % 10^9) = 98765ms


STEP 3: Fetch user details (batch)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
user_ids = ["user_top1", "user_top2", ..., "user_top100"]

Query PostgreSQL:
SELECT user_id, username, avatar_url, country
FROM users
WHERE user_id IN (...100 user_ids...);

Time: <20ms (indexed lookup)


STEP 4: Return combined data
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Response:
{
  leaderboard: [
    {
      rank: 1,
      user_id: "user_top1",
      username: "CodeMaster",
      avatar: "https://...",
      country: "US",
      problems_solved: 892,
      total_runtime: "98.8s"
    },
    ...
  ],
  total_users: 20000000,
  last_updated: "2024-02-26T10:05:00Z"
}

Total latency: <30ms


For user's own rank:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ZREVRANK leaderboard:global "user_001"
â†’ Returns: 12344

ZSCORE leaderboard:global "user_001"
â†’ Returns: 451000123404

User sees: "Your rank: #12,345 / 20,000,000"
```

```
FLOW 5: Contest Real-Time Updates
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Weekly contest starts: 10,000 participants
        â”‚
        â–¼
Contest initialization:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Create contest leaderboard:
Key: contest:123:leaderboard

For each registered user:
ZADD contest:123:leaderboard 0 "user_001"
ZADD contest:123:leaderboard 0 "user_002"
...

All start with score 0


User solves problem during contest:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User_001 solves Problem A at t=15min (1 wrong attempt)
        â”‚
        â–¼
STEP 1: Calculate penalty
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
time_penalty = submission_time - contest_start + (20 Ã— wrong_attempts)
             = 15 + (20 Ã— 1)
             = 35 minutes

problems_solved = 1
score = (1 Ã— 10^12) + 35 = 1000000000035


STEP 2: Update contest leaderboard
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ZADD contest:123:leaderboard 1000000000035 "user_001"

Atomic operation
Instant rank update


STEP 3: Broadcast to viewers
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pub/Sub channel: contest:123:updates

PUBLISH contest:123:updates '{
  type: "SOLVE",
  user: "user_001",
  problem: "A",
  time: 15,
  new_rank: 245
}'

All 10,000 participants subscribed to channel
See update instantly: "user_001 solved Problem A!"


STEP 4: Leaderboard polling
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users' browsers poll every 5 seconds:
GET /api/contest/123/leaderboard?top=50

ZREVRANGE contest:123:leaderboard 0 49 WITHSCORES

Returns top 50 in <5ms
10,000 users Ã— 0.2 req/sec = 2,000 req/sec
Redis handles easily


Final standings:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
After 90 minutes, contest ends

Freeze leaderboard:
SET contest:123:frozen "true"

No more score updates accepted

Generate final results:
ZREVRANGE contest:123:leaderboard 0 9999 WITHSCORES

Top 3 users awarded:
- Virtual prize money
- Badge: "Contest Winner ğŸ¥‡"
- Profile highlight

Leaderboard archived to PostgreSQL for history
```

```
FLOW 6: Plagiarism Detection (Background Job)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Background service runs hourly:
        â”‚
        â–¼
STEP 1: Fetch recent AC submissions
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Query Cassandra:
SELECT submission_id, user_id, problem_id, code
FROM submissions_by_problem
WHERE problem_id = 42
AND submission_time > NOW() - 1 HOUR
AND result = 'AC';

Returns: 5,000 submissions for problem #42


STEP 2: Normalize code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each submission:
  - Remove comments
  - Remove whitespace
  - Normalize variable names (a â†’ var1, b â†’ var2)
  - Generate AST (Abstract Syntax Tree)
  - Hash AST structure


STEP 3: Compare submissions
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Use MinHash LSH (Locality Sensitive Hashing):

For each code:
  shingles = generate_shingles(code, k=5)
  minhash = MinHash(shingles)
  
Insert into LSH index:
  lsh.insert(submission_id, minhash)

Query similar submissions:
  candidates = lsh.query(minhash, threshold=0.9)

If similarity > 90%:
  Flag for manual review


STEP 4: Store in PostgreSQL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INSERT INTO plagiarism_cases
(submission_id_1, submission_id_2, similarity_score, status)
VALUES ('sub_abc', 'sub_xyz', 0.95, 'pending_review');

Human moderators review flagged cases
```

```
FLOW 7: Analytics Query (Business Intelligence)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Product team: "What's our daily active users trend?"
        â”‚
        â–¼
Query Cassandra with Spark:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SELECT 
  toDate(submission_time) as date,
  COUNT(DISTINCT user_id) as dau
FROM submissions_by_problem
WHERE submission_time >= '2024-01-01'
AND submission_time < '2024-03-01'
GROUP BY toDate(submission_time)
ORDER BY date;

Spark distributes query across Cassandra cluster
Processes 50M submissions
Returns daily aggregates

Time: 2-5 minutes (acceptable for analytics)


Advanced analytics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Which problems have highest WA rate?"

SELECT 
  problem_id,
  COUNT(*) as total_submissions,
  SUM(CASE WHEN result = 'WA' THEN 1 ELSE 0 END) as wrong_answers,
  (wrong_answers::float / total_submissions) as wa_rate
FROM submissions_by_problem
WHERE submission_time >= NOW() - 30 DAYS
GROUP BY problem_id
HAVING total_submissions > 1000
ORDER BY wa_rate DESC
LIMIT 20;

Identifies problems that are confusing
Product team can improve problem statements

Time: 5-10 minutes


"Language popularity by problem difficulty"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WITH submissions AS (
  SELECT s.problem_id, s.language, p.difficulty
  FROM submissions_by_problem s
  JOIN problems p ON s.problem_id = p.problem_id
  WHERE s.submission_time >= NOW() - 30 DAYS
)
SELECT 
  difficulty,
  language,
  COUNT(*) as count
FROM submissions
GROUP BY difficulty, language
ORDER BY difficulty, count DESC;

Results:
Easy problems: Python most popular (60%)
Hard problems: C++ most popular (45%)

Informs language support priorities
```

---

## Tradeoffs vs Other Databases

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           â”‚ THIS ARCH    â”‚ POSTGRES ALL â”‚ MONGO ALL    â”‚ MYSQL ALL    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Problem metadata queries  â”‚ PostgreSQLâœ“  â”‚ PostgreSQLâœ“  â”‚ MongoDB âœ“    â”‚ MySQL âœ“      â”‚
â”‚ Submission write throughputâ”‚ Cassandraâœ“  â”‚ 2K/sec maxâœ—  â”‚ 50K/sec      â”‚ 2K/sec maxâœ—  â”‚
â”‚ Historical submission queryâ”‚ Cassandraâœ“  â”‚ Slow (>1yr)  â”‚ Slow         â”‚ Slow (>1yr)  â”‚
â”‚ Leaderboard updates       â”‚ Redis âœ“      â”‚ Seconds âœ—    â”‚ Seconds âœ—    â”‚ Seconds âœ—    â”‚
â”‚ Real-time rank query      â”‚ Redis âœ“      â”‚ Impossibleâœ—  â”‚ Impossibleâœ—  â”‚ Impossibleâœ—  â”‚
â”‚ Complex problem queries   â”‚ PostgreSQLâœ“  â”‚ PostgreSQLâœ“  â”‚ Limited      â”‚ MySQL âœ“      â”‚
â”‚ Time-series partitioning  â”‚ Cassandraâœ“   â”‚ Manual       â”‚ Manual       â”‚ Manual       â”‚
â”‚ Horizontal scaling        â”‚ Native âœ“     â”‚ Sharding     â”‚ Native âœ“     â”‚ Sharding     â”‚
â”‚ Operational complexity    â”‚ HIGH         â”‚ LOW âœ“        â”‚ MEDIUM       â”‚ LOW âœ“        â”‚
â”‚ Cost at LeetCode scale    â”‚ MEDIUM       â”‚ HIGH         â”‚ HIGH         â”‚ HIGH         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Alternative Architectures (Why NOT Used)

```
ALTERNATIVE 1: All PostgreSQL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Single PostgreSQL database:
- Problems (works fine)
- Submissions (problems at scale)
- Leaderboard (materialized view)

Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ— 2,000 submissions/sec during contest overwhelms single DB
âœ— 182M submissions/year makes queries slow
âœ— Leaderboard materialized view refresh takes minutes
âœ— Calculating rank requires COUNT(*) over millions
âœ— Vacuum lag from constant inserts
âœ— Single point of failure

Works for: Small coding platforms (<10K users)


ALTERNATIVE 2: All MongoDB
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MongoDB for everything:
- Problems as documents (works)
- Submissions as documents (works)
- Leaderboard aggregation pipeline (slow)

Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ— Cannot efficiently query: "problems with tags X AND Y"
âœ— Aggregation pipeline for leaderboard takes seconds
âœ— No efficient way to get user's rank
âœ— Eventual consistency during contests
âœ— Sharding by problem_id scatters user's submissions

Works for: When queries are simple key-value lookups


ALTERNATIVE 3: All Cassandra
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Cassandra for everything:
- Problems (awkward - no JOINs)
- Submissions (perfect)
- Leaderboard (impossible)

Problems:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ— Cannot do complex problem queries (tags, companies, related)
âœ— No JOINs (must denormalize heavily)
âœ— Leaderboard requires scanning all users (impossible)
âœ— No sorted queries across partitions

Works for: Pure time-series append workloads only
```

---

## Why This Hybrid Is Optimal

```
ARCHITECTURAL PRINCIPLES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Match Database to Access Pattern
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Problems: Relational, complex queries â†’ PostgreSQL
Submissions: Time-series, append-heavy â†’ Cassandra
Leaderboard: Sorted rankings, real-time â†’ Redis


2. Separate Read and Write Paths
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write path:
  User submits â†’ Cassandra (async)
  
Read path:
  User views history â†’ Cassandra (sorted by time)
  User views leaderboard â†’ Redis (pre-computed)


3. Pre-compute Expensive Operations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Don't calculate rank on every request:
  âœ— SELECT COUNT(*) FROM users WHERE score > my_score
  
Pre-compute in Redis Sorted Set:
  âœ“ ZREVRANK leaderboard:global user_id


4. Use Right Tool for Each Job
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Test cases: Large files â†’ S3 (not database)
Code execution: Untrusted code â†’ Docker (isolated)
Real-time updates: WebSocket + Pub/Sub â†’ Redis
Analytics: Large aggregations â†’ Spark + Cassandra
```

---

## System Diagram

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   User Browser      â”‚
                        â”‚   (React App)       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                  â”‚                  â”‚
                â–¼                  â–¼                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Problem API  â”‚  â”‚ Submit API   â”‚  â”‚ Leaderboard  â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     API      â”‚
               â”‚                 â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                 â”‚                  â”‚
               â–¼                 â–¼                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ PostgreSQL  â”‚   â”‚   Kafka     â”‚   â”‚    Redis    â”‚
        â”‚             â”‚   â”‚  (queue)    â”‚   â”‚  (sorted    â”‚
        â”‚ - Problems  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   sets)     â”‚
        â”‚ - Test casesâ”‚          â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ - User statsâ”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Execution Workerâ”‚
                        â”‚   (Kafka â†’)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼            â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Docker 1 â”‚ â”‚ Docker 2 â”‚ â”‚ Docker N â”‚
              â”‚ (Python) â”‚ â”‚ (Java)   â”‚ â”‚ (C++)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚            â”‚            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Kafka       â”‚
                        â”‚  (results â†’)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼            â–¼            â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Cassandra â”‚ â”‚PostgreSQLâ”‚ â”‚  Redis   â”‚
              â”‚(update   â”‚ â”‚(update   â”‚ â”‚(update   â”‚
              â”‚submissn) â”‚ â”‚ stats)   â”‚ â”‚rank)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## One Line Summary

> **PostgreSQL stores problems and user stats because complex queries like "show Medium difficulty problems tagged 'Array' AND 'Hash Table' asked by Amazon with acceptance > 30%" require JOINs across problem_tags and problem_companies tables that NoSQL databases cannot express efficiently, while test case metadata lives in PostgreSQL but actual large test files (100MB inputs) are stored in S3 to avoid database bloat and leverage cheaper object storage at $0.023/GB/month versus $0.10+ for database storage â€” Cassandra stores submissions partitioned by user_id with clustering on submission_time because the append-heavy workload (2,000 submissions/second during contests) needs sequential writes to commit log that complete in <5ms versus PostgreSQL's B-tree index maintenance causing 500ms write latency spikes, and querying "show user's last 20 submissions" retrieves pre-sorted data from a single partition in 10ms versus PostgreSQL scanning 182 million rows even with indexes â€” Redis Sorted Sets maintain the global leaderboard because ZREVRANK returns user's rank among 20 million users in <2ms through O(log N) skip list traversal versus PostgreSQL's "SELECT COUNT(*) WHERE score > my_score" taking 5+ seconds to scan millions of rows, and ZINCRBY atomically updates rankings in <1ms when users solve problems versus recalculating the entire leaderboard materialized view taking minutes â€” code execution happens in isolated Docker containers with CPU/memory limits, read-only filesystems, and network disabled to prevent malicious code from accessing server files or consuming all resources, with untrusted submissions queued in Kafka and processed asynchronously by worker pools that mount test cases from S3, execute code with 10-second timeouts, compare outputs, and publish results back through Kafka to update Cassandra (submission history), PostgreSQL (user stats), and Redis (leaderboard) in parallel without blocking the submission API which returns immediately after queuing â€” this architecture handles 500K daily submissions across 3,000 problems with sub-50ms leaderboard queries, sub-second submission acknowledgments, and horizontal scaling by adding Cassandra nodes for write capacity, Redis replicas for read capacity, and Docker workers for execution parallelism.**